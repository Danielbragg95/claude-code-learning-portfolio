# Robots.txt for Claude Code Learning Portfolio
# This portfolio showcases AI-assisted development

User-agent: *
Allow: /

# Allow all search engines to crawl the entire site
Allow: /css/
Allow: /js/
Allow: /assets/

# Disallow crawling of development files
Disallow: /scripts/
Disallow: /.github/
Disallow: /node_modules/
Disallow: /package*.json

# Sitemap location
Sitemap: https://yourusername.github.io/claude-code-learning-portfolio/sitemap.xml

# Crawl delay (optional - adjust as needed)
Crawl-delay: 1

# Specific directives for major search engines
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

User-agent: Slurp
Allow: /

# Disallow any potential backup or temporary files
Disallow: /*.bak$
Disallow: /*.tmp$
Disallow: /*~$

# Allow social media crawlers for rich previews
User-agent: facebookexternalhit
Allow: /

User-agent: Twitterbot
Allow: /

User-agent: LinkedInBot
Allow: /